{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cfbee1-606c-411f-9803-f7176f34c402",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "MoA provide scripts to quickly reproduce some of the results, such as evaluations of [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval),\n",
    "[MT-Bench](https://github.com/lm-sys/FastChat), and [FLASK](https://github.com/kaistAI/FLASK).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc8f18-bdb3-4cf9-8217-3a33d3cd04b0",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17766df-cbbe-4420-93bd-dfd938a83e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Legacy editable install of alpaca-eval==0.6.2 from file:///app/alpaca_eval (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'wavedrom' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wavedrom'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install requirements\n",
    "!pip install -r requirements.txt -q\n",
    "!pip install together -q\n",
    "# 安裝 alpaca_eval\n",
    "!cd alpaca_eval && pip install -e . -q \n",
    "# 安裝 FastChat\n",
    "!cd FastChat && pip install -e \".[model_worker,llm_judge]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5921cda6-1b5e-47be-a149-cd05f5d42a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 設定 API Key\n",
    "os.environ['TOGETHER_API_KEY'] = xx\n",
    "os.environ['OPENAI_API_KEY'] = xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d46a8d-be9a-4d13-95d1-a6985a3c16e0",
   "metadata": {},
   "source": [
    "### List models of Together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27780516-7f8a-490d-ad2a-fcb65de31f36",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (500674518.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    #print(model['id'])\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import together\n",
    "import os\n",
    "\n",
    "# 設定 API Key\n",
    "together.api_key = os.environ['TOGETHER_API_KEY']\n",
    "\n",
    "# 列出所有模型\n",
    "models = together.Models.list()\n",
    "\n",
    "# 印出模型 id\n",
    "for model in models:\n",
    "    #print(model['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d8b35-1542-4a7b-bcc6-7ca5700e722b",
   "metadata": {},
   "source": [
    "### 小於 10B 的模型整理結果\n",
    "```text\n",
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,\n",
    "google/gemma-2b-it,\n",
    "meta-llama/Llama-3-8b-chat-hf,\n",
    "meta-llama/Llama-3.2-3B-Instruct-Turbo,\n",
    "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo,\n",
    "mistralai/Mistral-7B-Instruct-v0.1,\n",
    "Qwen/Qwen2.5-7B-Instruct-Turbo,\n",
    "scb10x/scb10x-llama3-1-typhoon2-8b-instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76867c3e-ff97-4e09-9a8a-f53f3fc38c04",
   "metadata": {},
   "source": [
    "### Run AlpacaEval 2 Small models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ed478-13af-411b-b052-89b2689a12fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-30 01:53:54.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1m`reference_models` provided: ['deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'google/gemma-2b-it', 'meta-llama/Llama-3-8b-chat-hf', 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'mistralai/Mistral-7B-Instruct-v0.1']. Will generate reference responses on-the-fly.\u001b[0m\n",
      "\u001b[32m2025-04-30 01:53:54.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mStart.\u001b[0m\n",
      "Map:   0%|                             | 3/805 [01:24<6:28:04, 29.03s/ examples]"
     ]
    }
   ],
   "source": [
    "!export DEBUG=1\n",
    "\n",
    "# Change models to SLMs\n",
    "reference_models=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,google/gemma-2b-it,meta-llama/Llama-3-8b-chat-hf,Qwen/Qwen2.5-7B-Instruct-Turbo,mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "!python generate_for_alpaca_eval.py \\\n",
    "    --model=\"Qwen/Qwen2.5-7B-Instruct-Turbo\" \\\n",
    "    --output-path=\"outputs/test2.json\" \\\n",
    "    --reference-models=\"$reference_models\" \\\n",
    "    --rounds 1 \\\n",
    "    --num-proc 1\n",
    " \n",
    "#    --reference-models=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" \\\n",
    "#    --reference-models=\"$reference_models\" \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994623d-482c-4239-bd69-1414aeb3ec05",
   "metadata": {},
   "source": [
    "### Use vllm server to do evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07a042cb-1e01-496e-bd8f-3c4ff4b3a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the Qwen/Qwen2.5-7B-Instruct-Turbo-together outputs.\n",
      "INFO:root:Creating the annotator from `/app/vllm_annotator.yaml`.\n",
      "INFO:root:Saving annotations to `/app/annotations_seed0_vllm_annotator.json`.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]INFO:root:Annotating 128 examples with vllm-annotator\n",
      "INFO:root:Using `openai_completions` on 128 prompts using /app/models/DeepSeek-R1-Distill-Qwen-1.5B.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/alpaca_eval\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/main.py\", line 611, in main\n",
      "    fire.Fire(evaluate)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 135, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 468, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/main.py\", line 155, in evaluate\n",
      "    annotations = annotator.annotate_head2head(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/annotators/pairwise_evaluator.py\", line 237, in annotate_head2head\n",
      "    out = self.__call__(df_to_annotate, **decoding_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/annotators/base.py\", line 218, in __call__\n",
      "    df_annotated = self._annotate(curr_df_to_annotate, **decoding_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/annotators/base.py\", line 336, in _annotate\n",
      "    curr_annotated = self.annotators[annotator](\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/annotators/base.py\", line 718, in __call__\n",
      "    completions = self.fn_completions(prompts=prompts, **self.completions_kwargs, **decoding_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/decoders/openai.py\", line 119, in openai_completions\n",
      "    prompts = [utils.prompt_to_chatml(prompt) for prompt in prompts]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/decoders/openai.py\", line 119, in <listcomp>\n",
      "    prompts = [utils.prompt_to_chatml(prompt) for prompt in prompts]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/alpaca_eval/utils.py\", line 632, in prompt_to_chatml\n",
      "    assert prompt.startswith(start_token)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!alpaca_eval \\\n",
    "    --model_outputs outputs/test.json \\\n",
    "    --reference_outputs alpaca_eval/results/gpt4_1106_preview/model_outputs.json \\\n",
    "    --annotators_config /app/vllm_annotator.yaml \\\n",
    "    --output_path leaderboard_vllm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9487f7e9-5b3e-47e4-aa45-fd1349e7afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating the Qwen/Qwen2.5-7B-Instruct-Turbo-together outputs.\n",
      "INFO:root:Creating the annotator from `weighted_alpaca_eval_gpt4_turbo`.\n",
      "INFO:root:Saving annotations to `/usr/local/lib/python3.10/dist-packages/alpaca_eval/evaluators_configs/weighted_alpaca_eval_gpt4_turbo/annotations_seed0_configs.json`.\n",
      "Annotation chunk:   0%|                                   | 0/7 [00:00<?, ?it/s]INFO:root:Annotating 128 examples with weighted_alpaca_eval_gpt4_turbo\n",
      "INFO:root:Using `openai_completions` on 128 prompts using gpt-4-1106-preview.\n",
      "INFO:root:Kwargs to completion: {'model': 'gpt-4-1106-preview', 'temperature': 1, 'logprobs': True, 'top_logprobs': 5, 'is_chat': True}. num_procs=5\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   0%|                                   | 0/128 [00:00<?, ?it/s]\u001b[AWARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   1%|▏                          | 1/128 [00:02<04:37,  2.19s/it]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:   5%|█▍                         | 7/128 [00:05<01:21,  1.49it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  14%|███▋                      | 18/128 [00:05<00:22,  4.80it/s]\u001b[AWARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  18%|████▋                     | 23/128 [00:06<00:22,  4.66it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "prompt_batches:  20%|█████▎                    | 26/128 [00:06<00:19,  5.37it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  23%|█████▉                    | 29/128 [00:07<00:21,  4.52it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  24%|██████▎                   | 31/128 [00:07<00:19,  4.97it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.622000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.056000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  26%|██████▋                   | 33/128 [00:08<00:18,  5.10it/s]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.348000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.834000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  27%|███████                   | 35/128 [00:08<00:20,  4.47it/s]\u001b[AWARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.230000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.842000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.620000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.138000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.736000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.464000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29715, Requested 1442. Please try again in 2.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29654, Requested 1437. Please try again in 2.182s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29564, Requested 1665. Please try again in 2.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.251000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.482000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.722000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.238000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.600000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29099, Requested 1482. Please try again in 1.162s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.440000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.684000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.690000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.830000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  29%|███████▌                  | 37/128 [00:17<01:46,  1.17s/it]\u001b[AWARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.706000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29616, Requested 1437. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.102000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.950000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29388, Requested 1665. Please try again in 2.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29368, Requested 1677. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.921000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.874000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29398, Requested 1482. Please try again in 1.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.876000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.852000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.672000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.684000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.648000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.228000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.092000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.576000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28940, Requested 1437. Please try again in 754ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.752000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28718, Requested 1665. Please try again in 766ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28695, Requested 1677. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29692, Requested 1482. Please try again in 2.348s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.985000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.774000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.722000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.672000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.520000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.426000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.688000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.680000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.528000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.508000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.904000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29716, Requested 1482. Please try again in 2.396s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29657, Requested 1533. Please try again in 2.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29392, Requested 1665. Please try again in 2.114s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29386, Requested 1677. Please try again in 2.126s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.126000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.452000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.440000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.122000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.134000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.986000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.920000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.224000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.216000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.904000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.218000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29744, Requested 1482. Please try again in 2.452s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29688, Requested 1533. Please try again in 2.442s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29340, Requested 1665. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29328, Requested 1677. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.534000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.752000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.742000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.726000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.736000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.810000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.958000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.568000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.950000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.954000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.955000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29734, Requested 1482. Please try again in 2.432s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29463, Requested 1533. Please try again in 1.992s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29338, Requested 1665. Please try again in 2.006s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.106000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29326, Requested 1677. Please try again in 2.006s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.248000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.234000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.260000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.234000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.766000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.774000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.768000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29573, Requested 1665. Please try again in 2.476s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29560, Requested 1677. Please try again in 2.474s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.302000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.298000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.818000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.868000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.887000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.626000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29889, Requested 1677. Please try again in 3.132s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "\n",
      "prompt_batches:  30%|███████▋                  | 38/128 [01:03<11:51,  7.90s/it]\u001b[AINFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.398000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.444000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.268000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.710000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.484000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.620000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29370, Requested 1677. Please try again in 2.094s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.460000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.716000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.644000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "prompt_batches:  35%|█████████▏                | 45/128 [01:18<06:10,  4.46s/it]\u001b[AWARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.382000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.756000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.512000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.496000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.628000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.650000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.990000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28586, Requested 1996. Please try again in 1.164s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.314000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.472000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.116000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.296000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.106000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.068000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.462000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29347, Requested 1800. Please try again in 2.294s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.260000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.148000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29195, Requested 1379. Please try again in 1.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.818000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29368, Requested 1996. Please try again in 2.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.920000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29132, Requested 1634. Please try again in 1.532s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.340000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.308000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.568000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.570000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.100000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.235000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.348000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.354000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28907, Requested 1778. Please try again in 1.37s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28890, Requested 1800. Please try again in 1.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.378000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29398, Requested 1634. Please try again in 2.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.768000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29039, Requested 1996. Please try again in 2.07s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.432000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.422000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.356000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.944000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.883000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.886000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.954000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.082000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29753, Requested 1778. Please try again in 3.062s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29741, Requested 1800. Please try again in 3.082s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.082000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.748000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.180000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.206000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29115, Requested 1770. Please try again in 1.77s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.746000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28963, Requested 1996. Please try again in 1.917s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.891000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.866000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.876000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.080000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.018000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.720000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29582, Requested 1778. Please try again in 2.72s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29565, Requested 1800. Please try again in 2.73s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.314000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.212000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.988000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.993000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.670000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28863, Requested 1770. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28814, Requested 1996. Please try again in 1.62s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.754000 seconds\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.764000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.434000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.030000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.388000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.256000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29350, Requested 1778. Please try again in 2.256s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 29324, Requested 1800. Please try again in 2.248s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.106000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "WARNING:root:/usr/local/lib/python3.10/client_configs/openai_configs.yaml wasn't found. We are using environment variables to construct the client configs.This is the old and non-recommended way of doing it. Please see `client_configs/README.md` for the recommended way of specifying client configs.\n",
      "INFO:root:Using OAI client number 1 out of 1.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.244000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.766000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.782000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:root:OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-V5CtQV484rYHam7Sc8p3hZUi on tokens per min (TPM): Limit 30000, Used 28497, Requested 1770. Please try again in 534ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "INFO:root:Sleeping 2 before retrying to call openai API...\n",
      "^C\n",
      "prompt_batches:  48%|████████████▍             | 61/128 [02:12<02:25,  2.17s/it]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.646000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.650000 seconds\n"
     ]
    }
   ],
   "source": [
    "!alpaca_eval \\\n",
    "    --model_outputs outputs/test.json \\\n",
    "    --reference_outputs alpaca_eval/results/gpt4_1106_preview/model_outputs.json \\\n",
    "    --output_path leaderboard_vllm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
